{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 第4章 word2vec的高速化\n",
    "\n",
    "随着语料库中处理的词汇量的增加，计算量也随之增加。\n",
    "当词汇量达到一定程度之后，上一章的 CBOW 模型的计算就会花费过多的时间。\n",
    "\n",
    "简单的 word2vec 进行两点改进：引入Embedding层和Negative Sampling的新的损失函数。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 word2vec的改进1\n",
    "\n",
    "![](../images/图4-1.上一章中实现的CBOW模型.PNG)\n",
    "图4-1.上一章中实现的CBOW模型\n",
    "\n",
    "![](../images/图4-2.假设词汇量为100万个时的CBOW模型.PNG)\n",
    "图4-2.假设词汇量为100万个时的CBOW模型\n",
    "\n",
    "以下两个地方的计算会出现瓶颈。\n",
    "* 输入层的 one-hot 表示和权重矩阵 $W_in$ 的乘积（4.1 节解决，引入Embedding层解决）\n",
    "* 中间层和权重矩阵 $W_out$ 的乘积以及 Softmax 层的计算（4.2 节解决，引入Negative Sampling新损失函数）\n",
    "\n",
    "### 4.1.1 Embedding层\n",
    "\n",
    "![](../images/图4-3.one-hot表示的上下文和MatMul层的权重的乘积.PNG)\n",
    "图4-3.one-hot表示的上下文和MatMul层的权重的乘积\n",
    "\n",
    "创建一个从权重参数中抽取“单词 ID 对应行（向量）”的层，这里我们称之为 Embedding 层。\n",
    "顺便说一句，Embedding 来自“词嵌入”（word embedding）这一术语。也就是说，\n",
    "在这个 Embedding 层存放词嵌入（分布式表示）。\n",
    "\n",
    "Embedding层：从权重参数中抽取“单词ID对应行（向量）”的层。\n",
    "Embedding来自“词嵌入”（word embedding）这一术语，在Embedding层放入词嵌入（分布式表示）。\n",
    "\n",
    "在自然语言处理领域，单词的密集向量表示称为**词嵌入**（word embedding）或者单词的**分布式表示**（distributed representation）。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.2 Embedding层的实现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14],\n       [15, 16, 17],\n       [18, 19, 20]])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.arange(21).reshape(7, 3)\n",
    "W"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T22:15:35.834953Z",
     "end_time": "2023-07-06T22:15:35.962518Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "array([6, 7, 8])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[2]  # 从这个权重中取出某个特定的行"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T22:15:47.656808Z",
     "end_time": "2023-07-06T22:15:47.667777Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 3,  4,  5],\n       [ 0,  1,  2],\n       [ 9, 10, 11],\n       [ 0,  1,  2]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.array([1, 0, 3, 0])\n",
    "W[idx]  # 从这个权重中取出多行"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T22:16:26.272423Z",
     "end_time": "2023-07-06T22:16:26.281399Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Embedding 层实现如下所示（ [common/layers.py](../common/layers.py)）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Embedding 层的正向传播只是从权重矩阵 $W$ 中提取特定的行，并将该特定行的神经元原样传给下一层。\n",
    "因此，在反向传播时，从上一层（输出侧的层）传过来的梯度将原样传给下一层（输入侧的层）。\n",
    "不过，从上一层传来的梯度会被应用到权重梯度 dW 的特定行（idx）。\n",
    "\n",
    "![](../images/图4-4.Embedding层的正向传播和反向传播处理的概要（Embedding层记为Embed）.PNG)\n",
    "图4-4.Embedding层的正向传播和反向传播处理的概要（Embedding层记为Embed）\n",
    "\n",
    "将 word2vec（CBOW 模型）的实现中的输入侧的 MatMul 层换成 Embedding 层。这样\n",
    "一来，既能减少内存使用量，又能避免不必要的计算。\n",
    "\n",
    "![](../images/图4-5.当idx数组的元素中出现相同的行号时，简单地将dh的行写入对应位置就会有问题.PNG)\n",
    "图4-5.当idx数组的元素中出现相同的行号时，简单地将dh的行写入对应位置就会有问题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 word2vec的改进2\n",
    "\n",
    "使用Negative Simpling（**负采样**）替代Softmax，无论词汇量有多大，都可以使计算量保持较低或恒定。\n",
    "\n",
    "### 4.2.1 中间层之后的计算问题\n",
    "\n",
    "![](../images/图4-6.词汇量为100万个时的word2vec：上下文是you和goodbye，目标词是say.PNG)\n",
    "图4-6　词汇量为100万个时的word2vec：上下文是you和goodbye，目标词是say\n",
    "\n",
    "下两个地方需要很多计算时间。\n",
    "* 中间层的神经元和权重矩阵（$W_{out}$）的乘积\n",
    "* Softmax 层的计算\n",
    "\n",
    "很有必要将矩阵乘积计算“轻量化”。\n",
    "\n",
    "### 4.2.2 从多分类到二分类\n",
    "\n",
    "用二分类（binary classification）拟合多分类（multiclass classification），这是理解负采样的重点。\n",
    "\n",
    "![](../images/图4-7.仅计算目标词的得分的神经网络.PNG)\n",
    "图4-7.仅计算目标词的得分的神经网络\n",
    "\n",
    "![](../images/图4-8.计算say对应的列向量和中间层的内积（图中的“dot”指内积运算）.PNG)\n",
    "图4-8.计算say对应的列向量和中间层的内积（图中的“dot”指内积运算）\n",
    "\n",
    "\n",
    "### 4.2.3 sigmoid函数和交叉熵误差\n",
    "\n",
    "sigmoid函数：\n",
    "$$\n",
    "    y = \\frac{1}{1 + \\exp(-x)} \\tag{4.2}\n",
    "$$\n",
    "\n",
    "![](../images/图4-9.Sigmoid层（左图）和sigmoid函数（右图）的图像.PNG)\n",
    "图4-9.Sigmoid层（左图）和sigmoid函数（右图）的图像\n",
    "\n",
    "\n",
    "$$\n",
    "    L = -\\sum_{k}t_k\\log y_k \\tag{1.7}\n",
    "$$\n",
    "\n",
    "用于 sigmoid 函数的损失函数\n",
    "$$\n",
    "    L = -(t\\log y+(1-t)\\log(1-y))    \\tag{4.3}\n",
    "$$\n",
    "y 是 sigmoid 函数的输出，t 是正确解标签，取值为 0 或 1\n",
    "\n",
    "\n",
    "![](../images/图4-10.Sigmoid%20层和%20Cross%20Entropy%20Error%20层的计算图。右图整合为了%20Sigmoid.PNG)\n",
    "图4-10.Sigmoid 层和 Cross Entropy Error 层的计算图。右图整合为了 Sigmoid with Loss层\n",
    "\n",
    "### 4.2.4 从多分类到二分类的实现\n",
    "\n",
    "![](../images/图4-11.进行多分类的CBOW模型的全貌图（Embedding层记为Embed）.PNG)\n",
    "图4-11.进行多分类的CBOW模型的全貌图（Embedding层记为Embed）\n",
    "\n",
    "![](../images/图4-12.进行二分类的CBOW模型的全貌图.PNG)\n",
    "图4-12.进行二分类的CBOW模型的全貌图\n",
    "\n",
    "将中间层的神经元记为 $h$\n",
    "\n",
    "\n",
    "引入 Embedding Dot 层，该层将图 4-12 中的 Embedding 层和 dot运算（内积）合并起来处理。\n",
    "使用这个层，图 4-12 的后半部分可以画成图 4-13。\n",
    "\n",
    "![](../images/图4-13.只关注图4-12的中间层之后的处理。使用Embedding%20Dot层合并Embedding.PNG)\n",
    "图4-13.只关注图4-12的中间层之后的处理。使用Embedding Dot层合并Embedding.PNG\n",
    "\n",
    "Embedding Dot 层的实现，这里我们将这个层实现为 EmbeddingDot 类（ [ch04/negative_sampling_layer.py](../ch04/negative_sampling_layer.py)）。\n",
    "\n",
    "![](../images/图4-14.Embedding%20Dot层中各个变量的值.PNG)\n",
    "图4-14.Embedding Dot层中各个变量的值\n",
    "\n",
    "### 4.2.5 负采样\n",
    "\n",
    "![](../images/图4-15.CBOW模型的中间层之后的处理：上下文是you和goodbye，此时目标词是say的概率为0.993（99.3%25）.PNG)\n",
    "图4-15.CBOW模型的中间层之后的处理：上下文是you和goodbye，此时目标词是say的概率为0.993（99.3%）\n",
    "\n",
    "![](../images/图4-16.如果say是正例（正确答案），则当输入say时使Sigmoid层的输出接近1，当输入say以外的单词时使输出接近0，我们要求的是这样的权重.PNG)\n",
    "图4-16.如果say是正例（正确答案），则当输入say时使Sigmoid层的输出接近1，当输入say以外的单词时使输出接近0，我们要求的是这样的权重\n",
    "\n",
    "\n",
    "只使用少数负例。这就是负采样方法的含义。\n",
    "\n",
    "负采样方法既可以求将正例作为目标词时的损失，同时也可\n",
    "以采样（选出）若干个负例，对这些负例求损失。然后，将这些数据（正例\n",
    "和采样出来的负例）的损失加起来，将其结果作为最终的损失。\n",
    "\n",
    "\n",
    "![](../images/图4-17.负采样的例子（只关注中间层之后的处理，画出基于层的计算图）.PNG)\n",
    "图4-17.负采样的例子（只关注中间层之后的处理，画出基于层的计算图）\n",
    "\n",
    "### 4.2.6 负采样的采样方法\n",
    "\n",
    "![](../images/图4-18.根据概率分布多次进行采样的例子.PNG)\n",
    "图4-18.根据概率分布多次进行采样的例子\n",
    "\n",
    "处理好高频单词才能获得更好的结果。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "9"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 Python 来说明基于概率分布的采样\n",
    "import numpy as np\n",
    "\n",
    "# 从 0-9 随机选择一个数字\n",
    "np.random.choice(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-20T08:59:29.419996Z",
     "end_time": "2023-07-20T08:59:29.426976Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'goodbye'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从words列表中随机选择一个元素\n",
    "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
    "np.random.choice(words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-20T09:01:04.982226Z",
     "end_time": "2023-07-20T09:01:04.988210Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['goodbye', 'goodbye', 'I', 'goodbye', 'you'], dtype='<U7')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 有放回采样5次\n",
    "np.random.choice(words, size=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-20T09:01:55.545814Z",
     "end_time": "2023-07-20T09:01:55.562230Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['I', 'say', '.', 'hello', 'you'], dtype='<U7')"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 无放回采样5次\n",
    "np.random.choice(words, size=5, replace=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-20T09:02:42.511690Z",
     "end_time": "2023-07-20T09:02:42.566253Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'you'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基于概率分布进行采样\n",
    "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
    "np.random.choice(words, p=p)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-20T09:03:57.191462Z",
     "end_time": "2023-07-20T09:03:57.204245Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "    P'(w_i)=\\frac{P(w_i)^{0.75}}{\\sum_j^nP(w_j)^{0.75}} \\tag{4.4}\n",
    "$$\n",
    "\n",
    "对原来的概率分布的各个元素取 0.75 次方，防止低频单词被忽略。更准确地说，通过取 0.75 次方，低频单词的概率将稍微变高。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64196878 0.33150408 0.02652714]\n"
     ]
    }
   ],
   "source": [
    "p = [0.7, 0.29, 0.01]\n",
    "new_p = np.power(p, 0.75)\n",
    "new_p /= np.sum(new_p)\n",
    "print(new_p)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-20T15:42:42.272923Z",
     "end_time": "2023-07-20T15:42:42.299877Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [2 4]\n",
      " [3 1]]\n"
     ]
    }
   ],
   "source": [
    "from negative_sampling_layer import *\n",
    "\n",
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1, 3, 0])\n",
    "negative_sample = sampler.get_negative_sampler(target)\n",
    "print(negative_sample)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-22T19:14:01.004944Z",
     "end_time": "2023-07-22T19:14:01.013920Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2.7 负采样的实现\n",
    "\n",
    "实现负采样。我们把它实现为 NegativeSamplingLoss 类，（[ch04/negative_sampling_layer.py](../ch04/negative_sampling_layer.py)）。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 改进版word2vec的学习\n",
    "\n",
    "### 4.3.1 CBOW模型的实现\n",
    "\n",
    "将上下文部分扩展为可以处理任意的窗口大小。（[ch04/cbow.py](../ch04/cbow.py)）。\n",
    "\n",
    "![](../images/图4-19.用单词ID表示上下文和目标词的例子：这里显示的是窗口大小为1的上下文.PNG)\n",
    "图4-19　用单词ID表示上下文和目标词的例子：这里显示的是窗口大小为1的上下文\n",
    "\n",
    "### 4.3.2 CBOW模型的学习代码\n",
    "\n",
    "（[ch04/train.py](../ch04/train.py)）\n",
    "\n",
    "### 4.3.3 CBOW模型的评价\n",
    "\n",
    "使用 word2vec 的单词的分布式表示，可以通过向量的加减法来解决类推问题。\n",
    "\n",
    "![](../images/图4-20.展示各个单词在单词向量空间上的相关性.PNG)\n",
    "图4-20.借助“man : woman = king : ?”这个类推问题，展示各个单词在单词向量空间上的相关性\n",
    "\n",
    "使用 word2vec 的单词的分布式表示，可以通过向量的加减法来解决类推问题。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 word2vec相关的其他话题\n",
    "\n",
    "### 4.4.1 word2vec的应用例\n",
    "\n",
    "在自然语言处理领域，单词的分布式表示之所以重要，原因就在于**迁移学习**（transfer learning）。\n",
    "迁移学习是指在某个领域学到的知识可以被应用于其他领域。\n",
    "\n",
    "单词的分布式表示的优点是可以将单词转化为固定长度的向量。另外，\n",
    "使用单词的分布式表示，也可以将文档（单词序列）转化为固定长度的向量。\n",
    "\n",
    "![](../images/图4-21.使用了单词的分布式表示的系统的处理流程.PNG)\n",
    "图4-21.使用了单词的分布式表示的系统的处理流程\n",
    "\n",
    "![](../images/图4-22.邮件的自动分类系统（情感分析）的例子.PNG)\n",
    "图4-22.邮件的自动分类系统（情感分析）的例子\n",
    "\n",
    "\n",
    "### 4.4.2 单词向量的评价方法\n",
    "\n",
    "![](../images/图4-23.基于类推问题的单词向量的评价结果.PNG)\n",
    "图4-23.基于类推问题的单词向量的评价结果"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 本章所学的内容\n",
    "\n",
    "* Embedding 层保存单词的分布式表示，在正向传播时，提取单词 ID对应的向量\n",
    "* 因为 word2vec 的计算量会随着词汇量的增加而成比例地增加，所以最好使用近似计算来加速\n",
    "* 负采样技术采样若干负例，使用这一方法可以将多分类问题转化为二分类问题进行处理\n",
    "* 基于 word2vec 获得的单词的分布式表示内嵌了单词含义，在相似的上下文中使用的单词在单词向量空间上处于相近的位置\n",
    "* word2vec 的单词的分布式表示的一个特性是可以基于向量的加减法运算来求解类推问题\n",
    "* word2vec 的迁移学习能力非常重要，它的单词的分布式表示可以应用于各种各样的自然语言处理任务"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
