{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 第1章-神经网络的复习\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 数学和Python的复习\n",
    "\n",
    "### 1.1.1 向量和矩阵\n",
    "\n",
    "向量是同时拥有大小和方向的量。向量可以表示为排成一排的数字集合，在 Python 实现中可以处理为一维数组。\n",
    "\n",
    "![](../images/图1-1.向量和矩阵的例子.PNG)\n",
    "图1-1.向量和矩阵的例子\n",
    "\n",
    "向量和矩阵可以分别用一维数组和二维数组表示。\n",
    "在矩阵中，将水平方向上的排列称为行（row），将垂直方向上的排列称为列（column）。\n",
    "\n",
    "将向量和矩阵扩展到 N 维的数据集合，就是**张量**。\n",
    "\n",
    "\n",
    "![](../images/图1-2.向量的表示方法.PNG)\n",
    "图1-2.向量的表示方法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([1, 2, 3])\n",
    "x.__class__ # 输出类名"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.203983Z",
     "end_time": "2023-07-13T16:08:05.347305Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(3,)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape # 向量的形状"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.343317Z",
     "end_time": "2023-07-13T16:08:05.402186Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim # 向量维度"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.357279Z",
     "end_time": "2023-07-13T16:08:05.403157Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 1)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([[1], [2], [3]])\n",
    "y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.374234Z",
     "end_time": "2023-07-13T16:08:05.417118Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.ndim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.391191Z",
     "end_time": "2023-07-13T16:08:05.418116Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1.2 矩阵的对应元素的运算\n",
    "\n",
    "+、-、*、/运算是对应多维数组中的元素（独立）进行的，这就是 NumPy 数组中的对应元素的运算。\n",
    "$A * B$区别于点乘 $A \\cdot B$ 。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1,  3,  5],\n       [ 7,  9, 11]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "X = np.array([[0, 1, 2], [3, 4, 5]])\n",
    "W + X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.408143Z",
     "end_time": "2023-07-13T16:08:05.423104Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0,  2,  6],\n       [12, 20, 30]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W * X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.421110Z",
     "end_time": "2023-07-13T16:08:05.517851Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1.3 广播\n",
    "\n",
    "![](../images/图1-3.广播的例子1.PNG)\n",
    "图1-3.广播的例子1：标量10被处理为2 x 2的矩阵"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[10, 20],\n       [30, 40]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "A * 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.440058Z",
     "end_time": "2023-07-13T16:08:05.518847Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[10, 20],\n       [30, 40]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[10, 10], [10, 10]])\n",
    "A * B"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.455018Z",
     "end_time": "2023-07-13T16:08:05.518847Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[30, 30],\n       [70, 70]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A, B)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.470975Z",
     "end_time": "2023-07-13T16:08:05.519844Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](../images/图1-4.广播的例子2.PNG)\n",
    "图1-4.广播的例子2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[10, 40],\n       [30, 80]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([10, 20])\n",
    "A * b"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.484939Z",
     "end_time": "2023-07-13T16:08:05.520842Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1.4 向量内积和矩阵乘积\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{x} = (x_1, \\cdots, x_n)\n",
    "    \\\\\n",
    "    \\boldsymbol{y} = (y_1, \\cdots, y_n)\n",
    "    \\\\\n",
    "    \\boldsymbol{x} \\cdot \\boldsymbol{y} = x_1y_1 + x_2y_2 + \\cdots + x_ny_n \\tag{1.1}\n",
    "$$\n",
    "\n",
    "向量内积是两个向量对应元素的乘积之和。\n",
    "\n",
    "![](../images/图1-5.矩阵乘积的计算方法.PNG)\n",
    "图1-5.矩阵乘积的计算方法\n",
    "\n",
    "向量内积直观地表示了“两个向量在多大程度上指向同一方向”。如\n",
    "果限定向量的大小为 1，当两个向量完全指向同一方向时，它们的\n",
    "向量内积是 1。反之，如果两个向量方向相反，则内积为 −1。\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "32"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 向量内积\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "np.dot(a, b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[19, 22],\n       [43, 50]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵乘积\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "np.dot(A, B)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1.5 矩阵形状的检查\n",
    "\n",
    "![](../images/图1-6.形状检查_在矩阵乘积中，要使对应维度的元素个数一致.PNG)\n",
    "图1-6.形状检查_在矩阵乘积中，要使对应维度的元素个数一致"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 神经网络的推理\n",
    "\n",
    "神经网络中进行的处理可以分为学习和推理两部分。\n",
    "\n",
    "### 1.2.1 神经网络的推理的全貌图\n",
    "\n",
    "神经网络就是一个函数。\n",
    "函数是将某些输入变换为某些输出的变换器，与此相同，神经网络也将输入变换为输出。\n",
    "\n",
    "![](../images/图1-7.神经网络的例子.PNG)\n",
    "图1-7.神经网络的例子\n",
    "\n",
    "$(x_1, x_2)$ 表示输入层的数据，\n",
    "用 $w_{11}$ 和 $w_{12}$ 表示权重，\n",
    "用 $b_1$ 表示偏置。\n",
    "\n",
    "隐藏层的第 1 个神经元就可以如下进行计算：\n",
    "$$\n",
    "    h_1 = x_1w_{11} + x_2w_{21} + b_1 \\tag{1.2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    (h_1, h_2, h_3, h_4) = (x_1, x_2)\n",
    "    \\begin{pmatrix}\n",
    "       w_{11}  &w_{12}  &w_{13}  &w_{14}  \\\\\n",
    "       w_{21}  &w_{22}  &w_{23}  &w_{24}  \\\\\n",
    "    \\end{pmatrix}\n",
    "    + (b_1, b_2, b_3, b_4)\n",
    "    \\tag{1.3}\n",
    "$$\n",
    "\n",
    "隐藏层的神经元被整理为 $(h_1, h_2, h_3, h_4)$\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{h} = \\boldsymbol{x}\\boldsymbol{W} + \\boldsymbol{b} \\tag{1.4}\n",
    "$$\n",
    "\n",
    "输入是 $\\boldsymbol{x}$\n",
    "隐藏神经元是 $\\boldsymbol{h}$\n",
    "权重是 $\\boldsymbol{W}$\n",
    "偏置是 $\\boldsymbol{b}$\n",
    "![](../images/图1-8.形状检查：确认对应维度的元素个数一致（省略偏置）.PNG)\n",
    "图1-8.形状检查：确认对应维度的元素个数一致（省略偏置）\n",
    "\n",
    "![](../images/图1-9.形状检查：mini-batch版的矩阵乘积（省略偏置）.PNG)\n",
    "图1-9.形状检查：mini-batch版的矩阵乘积（省略偏置）\n",
    "\n",
    "用 Python 写出 mini-batch 版的全连接层变换。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "W1 = np.random.randn(2, 4) # 权重\n",
    "b1 = np.random.randn(4) # 偏置\n",
    "x = np.random.randn(10, 2) # 输入\n",
    "h = np.dot(x, W1) + b1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "全连接层的变换是线性变换。激活函数赋予它“非线性”的效果。严格地讲，使用非线性的激活函数，可以增强神经网络的表现力。\n",
    "\n",
    "sigmoid 函数（sigmoid function）：\n",
    "\n",
    "$$\n",
    "    \\sigma(x) = \\frac{1}{1 + \\exp(-x)}  \\tag{1.5}\n",
    "$$\n",
    "\n",
    "![](../images/图1-10.sigmoid函数的图像.PNG)\n",
    "图1-10.sigmoid函数的图像\n",
    "\n",
    "sigmoid 函数接收任意大小的实数，输出 0 ～ 1 的实数。\n",
    "\n",
    "Python 来实现这个 sigmoid 函数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "a = sigmoid(h)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.random.randn(10, 2)\n",
    "W1 = np.random.randn(2, 4)\n",
    "b1 = np.random.randn(4)\n",
    "W2 = np.random.randn(4, 3)\n",
    "b2 = np.random.randn(3)\n",
    "h = np.dot(x, W1) + b1\n",
    "a = sigmoid(h)\n",
    "s = np.dot(a, W2) + b2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2.2 层的类化及正向传播的实现\n",
    "\n",
    "神经网络的推理所进行的处理相当于神经网络的正向传播。\n",
    "神经网络的学习，那时会按与正向传播相反的顺序传播数据（梯度），所以称为反向传播。\n",
    "\n",
    "本书在实现这些层时，制定以下“代码规范”。\n",
    "* 所有的层都有 forward() 方法和 backward() 方法\n",
    "* 所有的层都有 params（表保存权重和偏置等参数） 和 grads 实例变量\n",
    "\n",
    "forward() 方法和 backward() 方法分别对应正向传播和反向传播。\n",
    "params 使用列表保存权重和偏置等参数（参数可能有多个，所以用列表保存）。grads 以与 params 中的参数对应\n",
    "的形式，使用列表保存各个参数的梯度。\n",
    "\n",
    "[ch01/forward_net.py](../ch01/forward_net.py)\n",
    "\n",
    "![](../images/图1-11.要实现的神经网络的层结构.PNG)\n",
    "图1-11.要实现的神经网络的层结构\n",
    "\n",
    "输入 $\\boldsymbol{X}$ 经由 Affine 层、Sigmoid 层和 Affine 层后输出得分 $\\boldsymbol{S}$。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from ch01.forward_net import TwoLayerNet\n",
    "\n",
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 神经网络的学习\n",
    "\n",
    "### 1.3.1 损失函数\n",
    "\n",
    "在神经网络的学习中，为了知道学习进行得如何，需要一个指标。这个指标通常称为**损失**（loss）。\n",
    "失指示学习阶段中某个时间点的神经网络的性能。基于监督数据（学习阶段获得的正确解数据）和神经网络的预测结果，\n",
    "将模型的恶劣程度作为标量（单一数值）计算出来，得到的就是损失。\n",
    "\n",
    "计算神经网络的损失要使用**损失函数**（loss function）。\n",
    "进行多类别分类的神经网络通常使用**交叉熵误差**（cross entropy error）作为损失函数。交叉熵误差由神经网络输出的各类别的概率和监督标签求得。\n",
    "\n",
    "![](../images/图1-12.使用了损失函数的神经网络的层结构.PNG)\n",
    "图1-12.使用了损失函数的神经网络的层结构\n",
    "\n",
    "$\\boldsymbol{X}$ 是输入数据，$\\boldsymbol{t}$ 是监督标签，$\\boldsymbol{L}$ 是损失。\n",
    "Softmax层的输出是概率，该概率和监督标签被输入 Cross Entropy Error 层。\n",
    "\n",
    "Softmax 函数可由下式表示：\n",
    "$$\n",
    "    y_k = \\frac{\\exp(s_k)}{\\sum_{i=1}^{n}\\exp(s_i)} \\tag{1.6}\n",
    "$$\n",
    "\n",
    "\n",
    "交叉熵误差可由下式表示：\n",
    "$$\n",
    "    L = -\\sum_{k}t_k\\log y_k \\tag{1.7}\n",
    "$$\n",
    "\n",
    "在考虑了 mini-batch 处理的情况下，交叉熵误差可以由下式表示：\n",
    "$$\n",
    "    L = -\\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\log y_{nk} \\tag{1.8}\n",
    "$$\n",
    "这里假设数据有 $N$ 笔，$t{nk}$ 表示第 $n$ 笔数据的第 $k$ 维元素的值，$y{nk}$ 表示神\n",
    "经网络的输出，$t{nk}$ 表示监督标签。\n",
    "\n",
    "![](../images/图1-13.使用Softmax_with_Loss层输出损失.PNG)\n",
    "图1-13.使用Softmax_with_Loss层输出损失\n",
    "\n",
    "### 1.3.1 导数和梯度\n",
    "\n",
    "神经网络的学习的目标是找到损失尽可能小的参数。此时，导数和梯度非常重要。\n",
    "\n",
    "![](../images/图1-14.y=x2的导数表示x在各处的斜率.PNG)\n",
    "图1-14.$y=x^2$的导数表示$x$在各处的斜率\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{L}}{\\partial{x}} =\n",
    "    \\begin{pmatrix}\n",
    "        \\frac{\\partial{L}}{\\partial{x_1}},   &   \\frac{\\partial{L}}{\\partial{x_2}},   & \\cdots\n",
    "        & \\frac{\\partial{L}}{\\partial{x_3}}\n",
    "    \\end{pmatrix}\n",
    "    \\tag{1.9}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{L}}{\\partial{W}} =\n",
    "    \\begin{pmatrix}\n",
    "      \\frac{\\partial{L}}{\\partial{W_{11}}}   &  \\cdots &   \\frac{\\partial{L}}{\\partial{W_{1n}}} \\\\\n",
    "      \\vdots    &   \\ddots  \\\\\n",
    "      \\frac{\\partial{L}}{\\partial{W_{m1}}}   &   &   \\frac{\\partial{L}}{\\partial{W_{mn}}} \\\\\n",
    "    \\end{pmatrix}\n",
    "    \\tag{1.10}\n",
    "$$\n",
    "\n",
    "数学中梯度仅限于关于向量的导数。\n",
    "深度学习领域，一般也会定义关于矩阵和张量的导数，称为“梯度”。\n",
    "\n",
    "### 1.3.3 链式法则\n",
    "\n",
    "理解误差反向传播法的关键是**链式法则**。链式法则是复合函数的求导法则，其中复合函数是由多个函数构成的函数。\n",
    "\n",
    "考虑 $y = f(x)$ 和 $z = g(y)$ 这两个函数。如 $z = g(f(x))$ 所示，最终的输出 $z$ 由两个函数计算而来。此时，$z$ 关\n",
    "于 $x$ 的导数可以按下式求得：\n",
    "$$\n",
    "    \\frac{\\partial{z}}{\\partial{x}} = \\frac{\\partial{z}}{\\partial{y}} \\frac{\\partial{y}}{\\partial{x}}   \\tag{1.11}\n",
    "$$\n",
    "\n",
    "### 1.3.4 计算图\n",
    "\n",
    "![](../images/图1-15.z=x+y的计算图.PNG)\n",
    "图1-15.z = x + y的计算图\n",
    "\n",
    "梯度沿与正向传播相反的方向传播，这个反方向的传播称为**反向传播**。\n",
    "\n",
    "![](../images/图1-16.加法节点构成“复杂计算”的一部分.PNG)\n",
    "图1-16.加法节点构成“复杂计算”的一部分\n",
    "\n",
    "![](../images/图1-17.计算图的反向传播.PNG)\n",
    "图1-17.计算图的反向传播\n",
    "\n",
    "如图 1-17 所示，反向传播用蓝色的粗箭头表示，在箭头的下方标注传\n",
    "播的值。此时，传播的值是指最终的输出 L 关于各个变量的导数。\n",
    "\n",
    "![](../images/图1-18.加法节点的正向传播（左图）和反向传播（右图）.PNG)\n",
    "图1-18.加法节点的正向传播（左图）和反向传播（右图）\n",
    "\n",
    "#### 1.3.4.1 乘法节点\n",
    "\n",
    "![](../images/图1-19.乘法节点的正向传播（左图）和反向传播（右图）.PNG)\n",
    "图1-19.乘法节点的正向传播（左图）和反向传播（右图）\n",
    "\n",
    "\n",
    "#### 1.3.4.2 分支节点\n",
    "\n",
    "![](../images/图1-20.分支节点的正向传播（左图）和反向传播（右图）.PNG)\n",
    "图1-20.分支节点的正向传播（左图）和反向传播（右图）\n",
    "\n",
    "#### 1.3.4.3 Repeat 节点\n",
    "\n",
    "![](../images/图1-21.Repeat节点的正向传播（上图）和反向传播（下图）.PNG)\n",
    "图1-21.Repeat节点的正向传播（上图）和反向传播（下图）\n",
    "\n",
    "因为这个 Repeat 节点可以视为 N 个分支节点，所以它的反向传播可以通过 N 个梯度的总和求出。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(1, D) # 输入\n",
    "y = np.repeat(x, N, axis=0) # 正向传播\n",
    "\n",
    "dy = np.random.randn(N, D) # 假设的梯度\n",
    "dx = np.sum(dy, axis=0, keepdims=True) # 反向传播\n",
    "\n",
    "# 通过指定 keepdims=True，可以维持二维数组的维数\n",
    "# 当 keepdims=True 时，np.sum() 的结果的形状是 (1, D)；当keepdims=False 时，形状是 (D,)。"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.501893Z",
     "end_time": "2023-07-13T16:08:05.520842Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3.4.4 Sum节点\n",
    "\n",
    "![](../images/图1-22.Sum节点的正向传播（上图）和反向传播（下图）.PNG)\n",
    "图1-22.Sum节点的正向传播（上图）和反向传播（下图）\n",
    "\n",
    "Sum 节点的反向传播将上游传来的梯度分配到所有箭头上。这是加法节点的反向传播的自然扩展。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(N, D) # 输入\n",
    "y = np.sum(x, axis=0, keepdims=True) # 正向传播\n",
    "\n",
    "dy = np.random.randn(1, D) # 假设的梯度\n",
    "dx = np.repeat(dy, N, axis=0) # 反向传播"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.517851Z",
     "end_time": "2023-07-13T16:08:05.602622Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sum 节点和 Repeat 节点存在逆向关系。所谓逆向关系，是指 Sum 节点的正向传播相当于 Repeat 节点的反向传播，Sum 节点的反向传播相当于 Repeat 节点的正向传播。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3.4.5 MatMul节点\n",
    "\n",
    "![](../images/图1-23.MatMul节点的正向传播：矩阵的形状显示在各个变量的上方.PNG)\n",
    "图1-23.MatMul节点的正向传播：矩阵的形状显示在各个变量的上方\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{y} = \\boldsymbol{x}\\boldsymbol{W}\n",
    "$$\n",
    "\n",
    "\n",
    "关于 $x$ 的第 $i$ 个元素的导数  $\\frac{\\partial L}{\\partial x_i}$ ：\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial x_i} = \\sum_j  \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial x_i}\n",
    "    \\tag{1.12}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_i}$  表示变化程度，即当 $x_i$ 发生微小的变化时，$L$ 会有多\n",
    "大程度的变化。\n",
    "\n",
    "式（1.12）简化：\n",
    "$$\n",
    "    \\frac{\\partial y_j}{\\partial x_i} = W_{ij}\n",
    "$$\n",
    "代入式（1.12）\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial x_i} = \\sum_j \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i}\n",
    "    = \\sum_j \\frac{\\partial L}{\\partial y_j} W_{ij}\n",
    "    \\tag{1.13}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} W^T\n",
    "  \\tag{1.14}\n",
    "$$\n",
    "\n",
    "![](../images/图1-24.矩阵乘积的形状检查.PNG)\n",
    "图1-24.矩阵乘积的形状检查\n",
    "\n",
    "### 1.3.5 梯度的推导和反向传播的实现\n",
    "\n",
    "![](../images/图1-25.MatMul节点的反向传播.PNG)\n",
    "图1-25.MatMul节点的反向传播\n",
    "\n",
    "![](../images/图1-26.通过确认矩阵形状，推导反向传播的数学式.PNG)\n",
    "图1-26.通过确认矩阵形状，推导反向传播的数学式\n",
    "\n",
    "![](../images/图1-27.a=b和a%5B...%5D=b的区别：使用省略号时数据被覆盖，变量指向的内存地址不变.PNG)\n",
    "图1-27.a=b和a\\[...\\]=b的区别：使用省略号时数据被覆盖，变量指向的内存地址不变\n",
    "\n",
    "[common/layers.py](../common/layers.py)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.5 梯度推导和反向传播的实现\n",
    "\n",
    "#### 1.3.5.1 Sigmoid层\n",
    "\n",
    "sigmoid函数：\n",
    "$$\n",
    "    y = \\frac{1}{1 + \\exp(-x)}\n",
    "$$\n",
    "\n",
    "sigmoid函数的导数：\n",
    "$$\n",
    "    \\frac{\\partial y}{\\partial x} = y(1-y)\n",
    "    \\tag{1.15}\n",
    "$$\n",
    "\n",
    "![](../images/图1-28.Sigmoid层的计算图.PNG)\n",
    "图1-28 Sigmoid层的计算图\n",
    "\n",
    "#### 1.3.5.2 Affine层\n",
    "\n",
    "![](../images/图1-29.Affine层的计算图.PNG)\n",
    "图1-29.Affine层的计算图\n",
    "\n",
    "#### 1.3.5.3 Softmax with Loss 层\n",
    "\n",
    "![](../images/图1-30.Softmax_with_Loss层的计算图.PNG)\n",
    "图1-30.Softmax with Loss层的计算图"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.6 权重更新\n",
    "\n",
    "神经网络的学习按如下步骤进行：\n",
    "* 步骤1：mini-batch\n",
    "    从训练数据中随机选出多笔数据\n",
    "* 步骤2：计算梯度\n",
    "    基于误差反向传播法，计算损失函数关于各个权重参数的梯度\n",
    "* 步骤3：更新参数\n",
    "    使用梯度更新权重参数\n",
    "* 步骤4：重复\n",
    "    根据需要重复多次步骤1、步骤2和步骤3\n",
    "\n",
    "SGD 是一个很简单的方法。它将（当前的）权重朝梯度的（反）方向\n",
    "更新一定距离。如果用数学式表示，则有：\n",
    "$$\n",
    "    W \\leftarrow W - \\eta \\frac{\\partial L}{\\partial W}\n",
    "    \\tag{1.16}\n",
    "$$\n",
    "\n",
    "[common/optimizer.py](../common/optimizer.py)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 使用神经网络解决问题\n",
    "\n",
    "### 1.4.1 螺旋状数据集\n",
    "\n",
    "[ch01/show_spiral_dataset.py](../ch01/show_spiral_dataset.py)\n",
    "\n",
    "![](../images/图1-31.学习用的螺旋状数据集.PNG)\n",
    "图1-31.学习用的螺旋状数据集\n",
    "\n",
    "\n",
    "### 1.4.2 神经网络的实现\n",
    "\n",
    "[ch01/two_layer_net.py](../ch01/two_layer_net.py)\n",
    "\n",
    "### 1.4.3 学习用的代码\n",
    "\n",
    "[ch01/train_custom_loop.py](../ch01/train_custom_loop.py)\n",
    "\n",
    "![](../images/图1-32.损失的图形：横轴是学习的迭代次数（刻度值的10倍），竖轴是每10次迭代的平均损失.PNG)\n",
    "图1-32.损失的图形：横轴是学习的迭代次数（刻度值的10倍），竖轴是每10次迭代的平均损失\n",
    "\n",
    "![](../images/图1-33.学习后的神经网络的决策边界（用不同颜色描绘神经网络识别的各个类别的区域）.PNG)\n",
    "图1-33.学习后的神经网络的决策边界（用不同颜色描绘神经网络识别的各个类别的区域）\n",
    "\n",
    "将学习后的神经网络的区域划分（也称为决策边界）可视化\n",
    "\n",
    "### 1.4.4 Trainer类\n",
    "\n",
    "\n",
    "Trainer 类的代码在 [common/trainer.py](../common/trainer.py)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array([4, 8, 6, 7, 1, 5, 0, 2, 3, 9])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.permutation(10)  # 给定参数 N，该方法可以返回 0 到 N − 1 的随机序列"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.533809Z",
     "end_time": "2023-07-13T16:08:05.602622Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "array([4, 6, 0, 5, 7, 1, 9, 8, 2, 3])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.546773Z",
     "end_time": "2023-07-13T16:08:05.603620Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.5 计算的高速化\n",
    "\n",
    "### 1.5.1 位精度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "dtype('float64')"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(3)\n",
    "a.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.564724Z",
     "end_time": "2023-07-13T16:08:05.603620Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "dtype('float32')"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(3).astype(np.float32)\n",
    "b.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.580683Z",
     "end_time": "2023-07-13T16:08:05.603620Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "dtype('float32')"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.random.randn(3).astype('f')\n",
    "c.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.597638Z",
     "end_time": "2023-07-13T16:08:05.616585Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5.2 GPU（CuPy）\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 1., 2.],\n       [3., 4., 5.]], dtype=float32)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cupy as cp\n",
    "x = cp.arange(6).reshape(2, 3).astype('f')\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:08:05.610601Z",
     "end_time": "2023-07-13T16:08:06.765469Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 3., 12.], dtype=float32)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-13T16:21:42.410101Z",
     "end_time": "2023-07-13T16:21:42.780250Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 本章学内容\n",
    "\n",
    "* 神经网络具有输入层、隐藏层和输出层\n",
    "* 通过全连接层进行线性变换，通过激活函数进行非线性变换\n",
    "* 全连接层和 mini-batch 处理都可以写成矩阵计算\n",
    "* 使用误差反向传播法可以高效地求解神经网络的损失的梯度\n",
    "* 使用计算图能够将神经网络中发生的处理可视化，这有助于理解正向传播和反向传播\n",
    "* 在神经网络的实现中，通过将组件模块化为层，可以简化实现\n",
    "* 数据的位精度和 GPU 并行计算对神经网络的高速化非常重要"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
