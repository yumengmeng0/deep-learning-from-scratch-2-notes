{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 第2章-自然语言和单词的分布式表示\n",
    "\n",
    "## 2.1 什么是自然语言处理\n",
    "\n",
    "自然语言（natural language）\n",
    "\n",
    "自然语言处理（Natural Language Processing，NLP），顾名思义，就是处理自然语言的科学。\n",
    "它是一种能够让计算机理解人类语言的技术。\n",
    "\n",
    "自然语言处理的目标就是让计算机理解人说的话，进而完成对我们有帮助的事情。\n",
    "\n",
    "### 单词含义\n",
    "\n",
    "我们的语言是由文字构成的，而语言的含义是由单词构成的。换句话说，单词是含义的最小单位。\n",
    "\n",
    "单词含义的表示方法：\n",
    "* 基于同义词词典的方法\n",
    "* 基于计数的方法\n",
    "* 基于推理的方法（word2vec）\n",
    "\n",
    "[CS224d: Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 同义词字典\n",
    "\n",
    "![](../images/图2-1.同义词的例子：car、auto和automobile等都是表示“汽车”的同义词.PNG)\n",
    "图2-1.同义词的例子：car、auto和automobile等都是表示“汽车”的同义词\n",
    "\n",
    "![](../images/图2-2.根据各单词的含义，基于上位-下位关系形成的图.PNG)\n",
    "图2-2.根据各单词的含义，基于上位-下位关系形成的图\n",
    "\n",
    "### 2.2.1 WordNet\n",
    "\n",
    "在自然语言处理领域，最著名的同义词词典是 WordNet。\n",
    "WordNet 是普林斯顿大学于 1985 年开始开发的同义词词典，迄今已用于许多研究，\n",
    "并活跃于各种自然语言处理应用中。\n",
    "\n",
    "### 2.2.2 同义词词典的问题\n",
    "\n",
    "#### 难以顺应时代的变化\n",
    "#### 人力成本\n",
    "#### 无法表示单词的微妙差异"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 基于计数的方法\n",
    "\n",
    "**语料库**（corpus）就是大量的文本数据。\n",
    "语料库并不是胡乱收集数据，一般收集的都是用于自然语言处理研究和应用的文本数据。\n",
    "\n",
    "### 2.3.1 基于Python的语料库预处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'You say goodbye and I say hello .'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "text.lower()\n",
    "text = text.replace('.', ' .')\n",
    "text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-26T22:20:37.774329Z",
     "end_time": "2023-06-26T22:20:37.795273Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['You', 'say', 'goodbye', 'and', 'I', 'say', 'hello', '.']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text.split(' ')\n",
    "words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-26T22:21:06.265784Z",
     "end_time": "2023-06-26T22:21:06.276753Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-26T22:24:10.175789Z",
     "end_time": "2023-06-26T22:24:10.195702Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'You', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'I', 5: 'hello', 6: '.'}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-26T22:24:20.322256Z",
     "end_time": "2023-06-26T22:24:20.342262Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{'You': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'I': 4, 'hello': 5, '.': 6}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-26T22:24:24.642335Z",
     "end_time": "2023-06-26T22:24:24.661195Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 2, 3, 4, 1, 5, 6])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-26T22:25:34.538861Z",
     "end_time": "2023-06-26T22:25:34.712929Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 2, 3, 4, 5, 6, 7])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.util import *\n",
    "\n",
    "text = 'You say goodbye and I sat hello.'\n",
    "# corpus（语料库） 单词ID列表，word_to_id 单词到单词ID的字典，id_to_word 单词ID到单词的字典\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "corpus"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T14:27:09.058271Z",
     "end_time": "2023-06-27T14:27:09.072279Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "上述一系列处理实现为 preprocess() 函数（[common/util.py](../common/util.py)）。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.2 单词的分布式表示\n",
    "\n",
    "颜色也可以通过 RGB（Red/Green/Blue）三原色分别存在多少来表示。前者为不同的颜色赋予不同的名字，有\n",
    "多少种颜色，就需要有多少个不同的名字；后者则将颜色表示为三维向量。\n",
    "\n",
    "在单词领域构建紧凑合理的向量表示。\n",
    "能准确把握单词含义的向量表示。在自然语言处理领域，称为**分布式表示**。\n",
    "\n",
    "### 2.3.3 分布式假设\n",
    "\n",
    "**分布式假设**（distributional hypothesis）：某个单词的含义由它周围的单词组成。\n",
    "\n",
    "分布式假设所表达的理念非常简单。单词本身没有含义，单词含义由它所在的上下文（语境）形成。\n",
    "\n",
    "上下文指某个单词（关注词）周围的单词。\n",
    "\n",
    "![](../images/图2-3.窗口大小为2的上下文例子.PNG)\n",
    "图2-3.窗口大小为2的上下文例子。在关注goodbye时，将其左右各2个单词用作上下文\n",
    "\n",
    "将上下文的大小（即周围的单词有多少个）称为**窗口大小**（window size）。\n",
    "\n",
    "### 2.3.4 共现矩阵\n",
    "\n",
    "基于统计\\计数的方法：在关注某个单词的情况下，对它的周围出现多少次什么单词进行计数，然后再汇总。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from common.util import preprocess\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-14T11:38:57.352497Z",
     "end_time": "2023-07-14T11:38:57.918233Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](../images/图2-4.单词you的上下文.PNG)\n",
    "图2-4.单词you的上下文\n",
    "\n",
    "![](../images/图2-5.用表格表示单词you的上下文中包含的单词的频数.PNG)\n",
    "图2-5.用表格表示单词you的上下文中包含的单词的频数\n",
    "\n",
    "![](../images/图2-6.用表格表示单词say的上下文中包含的单词的频数.PNG)\n",
    "图2-6.用表格表示单词say的上下文中包含的单词的频数\n",
    "\n",
    "![](../images/图2-7.用表格汇总各个单词的上下文中包含的单词的频数.PNG)\n",
    "图2-7.用表格汇总各个单词的上下文中包含的单词的频数\n",
    "\n",
    "图 2-7 的表格呈矩阵状，所以称为**共现矩阵**（co-occurence matrix）。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0 1 0 1 0 0 0]\n",
      "[0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "C = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0],\n",
    "], dtype=np.int32)\n",
    "print(C[0]) # 单词ID为0的向量\n",
    "print(C[4])\n",
    "print(C[word_to_id['goodbye']]) # goodbye的向量"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-14T16:22:16.752917Z",
     "end_time": "2023-07-14T16:22:16.762890Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "直接从语料库生成共现矩阵的函数 create_co_matrix(corpus, vocab_size, window_size=1)，\n",
    "其中参数 corpus 是单词 ID 列表，参数 vocab_size 是词汇个数，window_size 是窗口大小（[common/util.py](../common/util.py)）。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.5 向量间的相似度\n",
    "\n",
    "测量向量间的相似度有很多方法，其中具有代表性的方法有向量内积或欧式距离等。\n",
    "\n",
    "**余弦相似度**（cosine similarity）\n",
    "$$\n",
    "    \\boldsymbol{x} = (x_1, x_2, x_3, \\cdots , x_n)\n",
    "    \\\\\n",
    "    \\boldsymbol{y} = (x_1, x_2, x_3, \\cdots , x_n)\n",
    "    \\\\\n",
    "    similarity(\\boldsymbol{x}, \\boldsymbol{y}) = \\frac{\\boldsymbol{x} \\boldsymbol{\\cdot} \\boldsymbol{y}}{ \\left\\| \\boldsymbol{x} \\right\\| \\left\\| \\boldsymbol{y} \\right\\|} =\n",
    "    \\frac{x_1y_1 + \\cdots + x_ny_n}{\\sqrt{x_1^2 + \\cdots + x_n^2}\\sqrt{y_1^2 + \\cdots + y_n^2}}\n",
    "    \\tag{2.1}\n",
    "$$\n",
    "\n",
    "余弦相似度直观地表示了“两个向量在多大程度上指向同一方向”。\n",
    "两个向量完全指向相同的方向时，余弦相似度为 1；完全指向相反\n",
    "的方向时，余弦相似度为 −1。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 余弦相似度，加eps防止除数为0\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / np.sqrt(np.sum(x**2) + eps) # x的正规化\n",
    "    ny = y / np.sqrt(np.sum(y**2) + eps) # y的正规化\n",
    "    return np.dot(nx, ny)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.6 相似单词的排序\n",
    "\n",
    "most_similar() 函数（ common/util.py）。\n",
    "\n",
    "[ch02/most_similar.py](../ch02/most_similar.py)\n",
    "\n",
    "most_similar(query, word_to_id, id_to_word, word_matrix, top=5)\n",
    "\n",
    "![](../images/表2-1.most_similar()函数的参数.PNG)\n",
    "表2-1.most_similar()函数的参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 基于计数的方法的改进\n",
    "\n",
    "### 2.4.1 点互信息（Pointwise Mutual Information，PMI）\n",
    "\n",
    "对于随机变量 $x$ 和 $y$，它们的PMI定义：\n",
    "$$\n",
    "    PMI(x,y) = \\log_2 \\frac{P(x,y)}{P(x)P(y)} \\tag{2.2}\n",
    "$$\n",
    "$P(x)$ 表示 $x$ 发生的概率，\n",
    "$P(y)$ 表示 $y$ 发生的概率，\n",
    "$P(x,y)$ 表示 $x$ 和 $y$ 同时发生的概率。\n",
    "PMI的值越高，相关性越强。\n",
    "\n",
    "\n",
    "使用共现矩阵（其元素表示单词共现的次数）来重写式(2.2)。\n",
    "这里，将共现矩阵表示为 $\\mathbf{C}$，将单词 $x$ 和 $y$ 的共现次数表示为 $\\mathbf{C}(x, y)$，将\n",
    "单词 $x$ 和 $y$ 的出现次数分别表示为 $\\mathbf{C}(x)$、$\\mathbf{C}(y)$，将语料库的单词数量记为\n",
    "$N$，则式 (2.2) 可以重写为：\n",
    "$$\n",
    "    PMI(x,y) = \\log_2 \\frac{P(x,y)}{P(x)P(y)}\n",
    "    = \\log_2 \\frac{\\frac{C(x,y)}{N}}{\\frac{C(x)}{N} \\frac{C(y)}{N}}\n",
    "    = \\log_2 \\frac{C(x,y) \\cdot N}{C(x)C(y)}\n",
    "    \\tag{2.3}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "    PMI(\"the\",\"car\") = \\log_2 \\frac{10 \\cdot 10000}{1000 \\cdot 20} \\approx 2.32 \\tag{2.4}\n",
    "    \\\\\n",
    "    PMI(\"car\",\"drive\") = \\log_2 \\frac{5 \\cdot 10000}{20 \\cdot 10} \\approx 7.97 \\tag{2.5}\n",
    "$$\n",
    "\n",
    "\n",
    "当两个单词共现次数是 $0$ 时，$log_20 = -\\infty$。\n",
    "实践中会使用**正点互信息**（Postive PMI，PPMI）。\n",
    "\n",
    "$$\n",
    "    PPMI(x,y) = \\max(0, PMI(x,y)) \\tag{2.6}\n",
    "$$\n",
    "\n",
    "实现将共现矩阵转化为PPMI 矩阵的函数。我们把这个函数称为 ppmi(C, verbose=False, eps=1e-8)\n",
    "（ [common/util.py](../common/util.py)）。\n",
    "\n",
    "将共现矩阵转化为 PPMI 矩阵，可以像下面这样进行实现（[ch02/ppmi.py](../ch02/ppmi.py)）。\n",
    "\n",
    "这个 PPMI 矩阵还是存在一个很大的问题，那就是随着语料库的词汇量增加，各个单词向量的维数也会增加。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4.2 降维\n",
    "\n",
    "**降维**（dimensionality reduction），在尽量保留“重要信息”的基础上减少向量维度。\n",
    "\n",
    "![](../images/图2-8.降维示意图.PNG)\n",
    "图2-8.降维示意图：发现重要的轴（数据分布广的轴），将二维数据表示为一维数据\n",
    "\n",
    "向量中的大多数元素为 0 的矩阵（或向量）称为稀疏矩阵（或稀疏向量）。\n",
    "从稀疏向量中找出重要的轴，用更少的维度对其进行重新表示。结果，\n",
    "稀疏矩阵就会被转化为大多数元素均不为0的密集矩阵。这个密集矩阵就是我们想要的单词的分布式表示。\n",
    "\n",
    "降维的方法有很多，这里我们使用奇异值分解（Singular Value Decomposition，SVD）\n",
    "\n",
    "**奇异值分解**（Singular Value Decomposition, SVD）\n",
    "\n",
    "SVD将任意矩阵分解为3个矩阵的乘积：\n",
    "$$\n",
    "    X = USV^T \\tag{2.7}\n",
    "$$\n",
    "\n",
    "SVD 将任意的矩阵 $X$ 分解为 $U$、$S$、$V$ 这 3 个矩阵的乘积，\n",
    "其中 $U$ 和 $V$ 是列向量彼此正交的正交矩阵，\n",
    "$S$ 是除了对角线元素以外其余元素均为 $0$ 的对角矩阵。\n",
    "\n",
    "![](../images/图2-9.基于SVD的矩阵变换（白色部分表示元素为0）.PNG)\n",
    "图2-9.基于SVD的矩阵变换（白色部分表示元素为0）\n",
    "\n",
    "![](../images/图2-10.基于SVD的降维示意图.PNG)\n",
    "图2-10.基于SVD的降维示意图\n",
    "\n",
    "### 2.4.3 基于SVD的降维\n",
    "\n",
    "创建一个共现矩阵，将其转化为 PPMI 矩阵，然后对其进行 SVD（ [ch02/count_method_small.py](../ch02/count_method_small.py)）\n",
    "\n",
    "![](../images/图2-11.对共现矩阵执行SVD，并在图上绘制各个单词的二维向量（i和goodbye重叠）.PNG)\n",
    "图2-11　对共现矩阵执行SVD，并在图上绘制各个单词的二维向量（i和goodbye重叠）\n",
    "\n",
    "* 根据操作系统的种类或Matplotlib版本的不同，输出的图可能和图2-11所有不同。\n",
    "\n",
    "### 2.4.4 PTB数据集\n",
    "\n",
    "Penn Treebank 语料库\n",
    "\n",
    "[ch02/show_ptb.py](../ch02/show_ptb.py)\n",
    "\n",
    "![](../images/图2-12.PTB语料库（文本文件）的例子.PNG)\n",
    "图2-12 PTB语料库（文本文件）的例子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4.5 基于PTB数据集的平评价\n",
    "\n",
    "[ch02/count_method_big.py](../ch02/count_method_big.py)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 小结\n",
    "\n",
    "## 本章所学的内容\n",
    "\n",
    "* 使用 WordNet 等同义词词典，可以获取近义词或测量单词间的相似度等\n",
    "* 使用同义词词典的方法存在创建词库需要大量人力、新词难更新等问题\n",
    "* 目前，使用语料库对单词进行向量化是主流方法\n",
    "* 近年来的单词向量化方法大多基于“单词含义由其周围的单词构成”这一分布式假设\n",
    "* 在基于计数的方法中，对语料库中的每个单词周围的单词的出现频数进行计数并汇总（= 共现矩阵）\n",
    "* 通过将共现矩阵转化为 PPMI 矩阵并降维，可以将大的稀疏向量转变为小的密集向量\n",
    "* 在单词的向量空间中，含义上接近的单词距离上理应也更近"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
